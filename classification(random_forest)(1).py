# -*- coding: utf-8 -*-
"""Classification(Random Forest) ML Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OYqGvSoy6f5duvTmjLbub2dx1HGt_VpE
"""

#we use a data set that contains information about customers of an
#online trading platform to classify whether a given customer's probability
#of churn will be high, medium, or low.

# Commented out IPython magic to ensure Python compatibility.
def warn(*args,**kwargs):
  pass

import warnings
warnings.warn=warn
warnings.filterwarnings('ignore',category=DeprecationWarning)
warn()


from tqdm import tqdm
import numpy as np
import pandas as pd
from itertools import accumulate
import matplotlib.pyplot as plt
import seaborn as sns

# Patching scikitplot to work with newer scipy versions
import scipy
if not hasattr(scipy, 'interp'):
    scipy.interp = np.interp

import scikitplot as skplt
# %matplotlib inline

from sklearn.preprocessing import StandardScaler,OneHotEncoder,LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score,classification_report

sns.set_context('notebook')
sns.set_style('whitegrid')
sns.set_palette('deep')



#Data exploration
df_churn=pd.read_csv('/content/churn.csv')
df_churn.head()

print("The dataset contains columns of the following data types : \n" + str(df_churn.dtypes))

print("The dataset contains following number of records for each of the columns: \n" +str(df_churn.count()))
#The count mismatch in the gender column(handled in the data preprocessing)

print("Each category within the churnrisk column has the count given below-")
print(df_churn.groupby(['CHURNRISK']).size())

#Chrunrisk Column representation for better understanding
index=['High','Medium','Low']
churn_plot=df_churn['CHURNRISK'].value_counts(sort=True,ascending=False).plot(kind='bar',figsize=(4,4),title="Total number of occurences of chrun risk"+str(df_churn['CHURNRISK'].count()),color=['#BB6B5A', '#8CCB9B', '#E5E88B'])

churn_plot.set_xlabel("Churn Risk")
churn_plot.set_ylabel("Frequency")
plt.show()

"preprocessing"
df_churn=df_churn.drop(['ID'], axis=1)
df_churn.head()

categoricalcol=['GENDER','STATUS','HOMEOWNER']
print("Categorical columns in the dataset are-")
print(categoricalcol)

impute_category=SimpleImputer(strategy='most_frequent')
onehot_encoder=OneHotEncoder(handle_unknown='ignore')

category_tranformer=Pipeline(steps=[('impute',impute_category),('onehot',onehot_encoder)])
#This combines both steps into a single pipeline that will:
#First fill missing values (impute step)
#Then convert categories to numbers (onehot step)

# Defining the numerical columns
numericalColumns = df_churn.select_dtypes(include=[np.number]).columns

print("Numerical columns : " )
print(numericalColumns)

scaler_numerical = StandardScaler()

numerical_transformer = Pipeline(steps=[('scale', scaler_numerical)])

preprocessorForCategoricalColumns = ColumnTransformer(transformers=[('cat', category_tranformer,
                                                                    categoricalcol) ],
                                            remainder="passthrough" )
preprocessorForAllColumns = ColumnTransformer(transformers=[('cat', category_tranformer, categoricalcol),
                                                            ('num', numerical_transformer, numericalColumns) ],
                                                remainder="passthrough" )

#. The transformation happens in the pipeline. Temporarily done here to show what intermediate value looks like
df_churn_pd_temp = preprocessorForCategoricalColumns.fit_transform(df_churn)

print("Data after transforming :")

print(df_churn_pd_temp)

df_churn_pd_temp_2 = preprocessorForAllColumns.fit_transform(df_churn)
print("Data after transforming :")
print(df_churn_pd_temp_2)

features=[]
features=df_churn.drop(['CHURNRISK'],axis=1)

label_churn=pd.DataFrame(df_churn,columns=['CHURNRISK'])

label_encoder=LabelEncoder()
label=df_churn['CHURNRISK']

label=label_encoder.fit_transform(label)
print("encoded value of churnrisk after label encoding:-"+str(label))

X_train, X_test, y_train, y_test = train_test_split(features, label, random_state=0)
print("Dimensions of datasets that will be used for training : Input features" + str(X_train.shape) + " Output label" + str(y_train.shape))
print("Dimensions of datasets that will be used for testing : Input features" + str(X_test.shape) + " Output label" + str(y_test.shape))

model_name = "Random Forest Classifier"

randomForestClassifier = RandomForestClassifier(n_estimators=60, max_depth=3, random_state=12)

rfc_model = Pipeline(steps=[('preprocessorAll', preprocessorForAllColumns), ('classifier', randomForestClassifier)])

# Build models

rfc_model.fit(X_train, y_train)

y_pred_rfc = rfc_model.predict(X_test)

# prompt: accuracy,classification report and confusin metrics of this model

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns


# Calculate accuracy
accuracy_rfc = accuracy_score(y_test, y_pred_rfc)
print(f"Accuracy of {model_name}: {accuracy_rfc:.4f}")

# Print classification report
print(f"\nClassification Report for {model_name}:\n")
print(classification_report(y_test, y_pred_rfc))

# Calculate confusion matrix
cm_rfc = confusion_matrix(y_test, y_pred_rfc)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_rfc, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title(f'Confusion Matrix for {model_name}')
plt.show()

print("checking for overfitting\n")

print("Train Accuracy:", accuracy_score(y_train, rfc_model.predict(X_train)))
print("Test Accuracy:", accuracy_score(y_test, rfc_model.predict(X_test)))

print('--------------------------------------------')
from sklearn.model_selection import cross_val_score
scores = cross_val_score(rfc_model, X_train, y_train, cv=5)
print("Cross-validation scores:", scores)
print("Mean CV score:", scores.mean())

import pickle

# Save the trained model
with open('random_forest_model.pkl', 'wb') as f:
    pickle.dump(randomForestClassifier, f)

